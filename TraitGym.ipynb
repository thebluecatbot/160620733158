{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TraitGym [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/songlab-cal/TraitGym/blob/main/TraitGym.ipynb)\n",
        "In this example we will load the Mendelian traits dataset and run variant effect prediction based on euclidean distance of GPN-Animal-Promoter embeddings of the reference and alternate sequences."
      ],
      "metadata": {
        "id": "muWargPyv5Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "5V9APVrtwLlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyfaidx s3fs git+https://github.com/songlab-cal/gpn.git\n",
        "!pip install -q -U transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II5CCc_mxEXj",
        "outputId": "33f119d4-e91e-45a1-dc83-630c8ad10bb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gpn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "s3fs 2025.3.2 requires fsspec==2025.3.2.*, but you have fsspec 2024.12.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.Seq import Seq\n",
        "from datasets import load_dataset\n",
        "import fsspec\n",
        "import gpn.model  # to register AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from pyfaidx import Fasta\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import average_precision_score\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "yFgUHIlJxs2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "3ef5244c-10e1-4247-f0d7-bc9f1a5a7fd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1e6839c72280>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m  \u001b[0;31m# to register AutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3.5.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"songlab/TraitGym\"\n",
        "dataset_config = \"mendelian_traits\"\n",
        "model_path = \"songlab/gpn-animal-promoter\"\n",
        "# in the paper we average with the predictions with the reverse complement\n",
        "# however for a quick evaluation you can omit this\n",
        "average_rc = False"
      ],
      "metadata": {
        "id": "G9JKGeFXOB5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "39wqWU2nw0th"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p3Vk6zRvPhQ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(dataset_path, dataset_config, split=\"test\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "QW1mpMscyOUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you just want a dataframe you can load it directly:\n",
        "# pd.read_parquet(\"hf://datasets/songlab/TraitGym/mendelian_traits_matched_9/test.parquet\")\n",
        "V = dataset.to_pandas()\n",
        "V"
      ],
      "metadata": {
        "id": "1DBiScBWyVis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "PyvRpy3-x9UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper to compute a VEP score\n",
        "class ModelVEP(torch.nn.Module):\n",
        "    def __init__(self, model_path):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_path,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids_ref=None,\n",
        "        input_ids_alt=None,\n",
        "    ):\n",
        "        embed_ref = self.model(input_ids=input_ids_ref).last_hidden_state.reshape(len(input_ids_ref), -1)\n",
        "        embed_alt = self.model(input_ids=input_ids_alt).last_hidden_state.reshape(len(input_ids_ref), -1)\n",
        "        return F.pairwise_distance(embed_ref, embed_alt)\n",
        "\n",
        "\n",
        "class ModelVEPAverageRC(torch.nn.Module):\n",
        "    def __init__(self, model_path):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_path,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "    def get_scores(self, input_ids_ref, input_ids_alt):\n",
        "        embed_ref = self.model(input_ids=input_ids_ref).last_hidden_state.reshape(len(input_ids_ref), -1)\n",
        "        embed_alt = self.model(input_ids=input_ids_alt).last_hidden_state.reshape(len(input_ids_ref), -1)\n",
        "        return F.pairwise_distance(embed_ref, embed_alt)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids_ref_fwd=None,\n",
        "        input_ids_alt_fwd=None,\n",
        "        input_ids_ref_rev=None,\n",
        "        input_ids_alt_rev=None,\n",
        "    ):\n",
        "        fwd = self.get_scores(input_ids_ref_fwd, input_ids_alt_fwd)\n",
        "        rev = self.get_scores(input_ids_ref_rev, input_ids_alt_rev)\n",
        "        return (fwd + rev) / 2"
      ],
      "metadata": {
        "id": "JID0TXVj5lv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "window_size = 512\n",
        "if average_rc:\n",
        "    model = ModelVEPAverageRC(model_path)\n",
        "else:\n",
        "    model = ModelVEP(model_path)"
      ],
      "metadata": {
        "id": "_-hrLbqwxqxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize dataset"
      ],
      "metadata": {
        "id": "BmoMd7eyx_eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Genome:\n",
        "    def __init__(self, path):\n",
        "        self.data = Fasta(fsspec.open(path, anon=True))\n",
        "\n",
        "    def __call__(self, chrom, start, end, strand=\"+\"):\n",
        "        res = self.data[chrom][start:end]\n",
        "        if strand == \"-\":\n",
        "            res = res.reverse.complement\n",
        "        return str(res)"
      ],
      "metadata": {
        "id": "qiLlhmcT1BxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genome = Genome(\"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\")"
      ],
      "metadata": {
        "id": "50tERpTd1jGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(seqs):\n",
        "    return tokenizer(\n",
        "        seqs,\n",
        "        padding=False,\n",
        "        truncation=False,\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=False,\n",
        "        return_special_tokens_mask=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "def get_tokenized_seq(vs):\n",
        "    # we convert from 1-based coordinate (standard in VCF) to\n",
        "    # 0-based, to use with Genome\n",
        "    chrom = np.array(vs[\"chrom\"])\n",
        "    n = len(chrom)\n",
        "    pos = np.array(vs[\"pos\"]) - 1\n",
        "    start = pos - window_size // 2\n",
        "    end = pos + window_size // 2\n",
        "    seq_fwd = [genome(\"chr\" + chrom[i], start[i], end[i]) for i in range(n)]\n",
        "    seq_fwd = np.array([list(seq.upper()) for seq in seq_fwd], dtype=\"object\")\n",
        "    assert seq_fwd.shape[1] == window_size\n",
        "    ref_fwd = np.array(vs[\"ref\"])\n",
        "    alt_fwd = np.array(vs[\"alt\"])\n",
        "    pos_fwd = window_size // 2\n",
        "\n",
        "    def prepare_output(seq, pos, ref, alt):\n",
        "        assert (seq[:, pos] == ref).all(), f\"{seq[:, pos]}, {ref}\"\n",
        "        seq_ref = seq\n",
        "        seq_alt = seq.copy()\n",
        "        seq_alt[:, pos] = alt\n",
        "        return (\n",
        "            tokenize([\"\".join(x) for x in seq_ref]),\n",
        "            tokenize([\"\".join(x) for x in seq_alt]),\n",
        "        )\n",
        "\n",
        "    res = {}\n",
        "    res[\"input_ids_ref\"], res[\"input_ids_alt\"] = prepare_output(seq_fwd, pos_fwd, ref_fwd, alt_fwd)\n",
        "    return res\n",
        "\n",
        "def get_tokenized_seq_average_rc(vs):\n",
        "    # we convert from 1-based coordinate (standard in VCF) to\n",
        "    # 0-based, to use with Genome\n",
        "    chrom = np.array(vs[\"chrom\"])\n",
        "    n = len(chrom)\n",
        "    pos = np.array(vs[\"pos\"]) - 1\n",
        "    start = pos - window_size // 2\n",
        "    end = pos + window_size // 2\n",
        "    seq_fwd = [genome(\"chr\" + chrom[i], start[i], end[i]) for i in range(n)]\n",
        "    seq_rev = [str(Seq(x).reverse_complement()) for x in seq_fwd]\n",
        "    seq_fwd = np.array([list(seq.upper()) for seq in seq_fwd], dtype=\"object\")\n",
        "    seq_rev = np.array([list(seq.upper()) for seq in seq_rev], dtype=\"object\")\n",
        "    assert seq_fwd.shape[1] == window_size\n",
        "    assert seq_rev.shape[1] == window_size\n",
        "    ref_fwd = np.array(vs[\"ref\"])\n",
        "    alt_fwd = np.array(vs[\"alt\"])\n",
        "    ref_rev = np.array([str(Seq(x).reverse_complement()) for x in ref_fwd])\n",
        "    alt_rev = np.array([str(Seq(x).reverse_complement()) for x in alt_fwd])\n",
        "    pos_fwd = window_size // 2\n",
        "    pos_rev = pos_fwd - 1 if window_size % 2 == 0 else pos_fwd\n",
        "\n",
        "    def prepare_output(seq, pos, ref, alt):\n",
        "        assert (seq[:, pos] == ref).all(), f\"{seq[:, pos]}, {ref}\"\n",
        "        seq_ref = seq\n",
        "        seq_alt = seq.copy()\n",
        "        seq_alt[:, pos] = alt\n",
        "        return (\n",
        "            tokenize([\"\".join(x) for x in seq_ref]),\n",
        "            tokenize([\"\".join(x) for x in seq_alt]),\n",
        "        )\n",
        "\n",
        "    res = {}\n",
        "    res[\"input_ids_ref_fwd\"], res[\"input_ids_alt_fwd\"] = prepare_output(seq_fwd, pos_fwd, ref_fwd, alt_fwd)\n",
        "    res[\"input_ids_ref_rev\"], res[\"input_ids_alt_rev\"] = prepare_output(seq_rev, pos_rev, ref_rev, alt_rev)\n",
        "    return res"
      ],
      "metadata": {
        "id": "2Nsgbz8A2Fo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if average_rc:\n",
        "    dataset.set_transform(get_tokenized_seq_average_rc)\n",
        "else:\n",
        "    dataset.set_transform(get_tokenized_seq)"
      ],
      "metadata": {
        "id": "zj4W7vHQ35Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "GLDcWQj94hu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference"
      ],
      "metadata": {
        "id": "-lz_YC-G539T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=tempfile.TemporaryDirectory().name,\n",
        "    per_device_eval_batch_size=128,\n",
        "    # pyfaidx does not allow multiple workers\n",
        "    # for longer jobs you can use the gpn.data.Genome class,\n",
        "    # which loads a local fasta file into memory and can use multiple\n",
        "    # workers, see e.g.\n",
        "    # https://github.com/songlab-cal/gpn/blob/main/gpn/ss/run_vep_embed_dist.py\n",
        "    dataloader_num_workers=0,\n",
        "    remove_unused_columns=False,\n",
        "    torch_compile=False,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "trainer = Trainer(model=model, args=training_args)\n",
        "preds = trainer.predict(test_dataset=dataset).predictions\n",
        "preds.shape"
      ],
      "metadata": {
        "id": "z7cS7RDC5DWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute metrics"
      ],
      "metadata": {
        "id": "ijChgnmI55fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V[\"score\"] = preds"
      ],
      "metadata": {
        "id": "84DkGrmPN5nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's compare to the precomputed scores\n",
        "# this is using RC averaging\n",
        "V[\"precomputed_score\"] = pd.read_parquet(\"hf://datasets/songlab/TraitGym/mendelian_traits_matched_9/features/GPN_final_EuclideanDistance.parquet\").score.values\n",
        "V[[\"score\", \"precomputed_score\"]].corr()"
      ],
      "metadata": {
        "id": "HheYRetBSher"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "sns.histplot(\n",
        "    data=V, x=\"score\", bins=30, hue=\"label\", stat=\"density\",\n",
        "    common_norm=False, common_bins=True,\n",
        ")\n",
        "sns.despine();"
      ],
      "metadata": {
        "id": "4y9p5gEtQpcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global AUPRC\n",
        "average_precision_score(V.label, V.score)"
      ],
      "metadata": {
        "id": "REMKfC6MTXiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUPRC by chrom\n",
        "res_by_chrom = []\n",
        "for chrom in V.chrom.unique():\n",
        "    V_chrom = V[V.chrom == chrom]\n",
        "    res_by_chrom.append([chrom, len(V_chrom), average_precision_score(V_chrom.label, V_chrom.score)])\n",
        "res_by_chrom = pd.DataFrame(res_by_chrom, columns=[\"chrom\", \"n\", \"AUPRC\"])\n",
        "res_by_chrom"
      ],
      "metadata": {
        "id": "KC4NnIFPUbVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted average\n",
        "\n",
        "def stat(df):\n",
        "    weight = df[\"n\"] / df[\"n\"].sum()\n",
        "    return (df[\"AUPRC\"] * weight).sum()\n",
        "\n",
        "stat(res_by_chrom)"
      ],
      "metadata": {
        "id": "8B9kgGTlVDHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_se(df, stat, n_bootstraps=1000):\n",
        "    df = pl.DataFrame(df)\n",
        "    return (\n",
        "        pl.Series([\n",
        "            stat(df.sample(len(df), with_replacement=True, seed=i))\n",
        "            for i in range(n_bootstraps)]\n",
        "        )\n",
        "        .std()\n",
        "    )\n",
        "\n",
        "bootstrap_se(res_by_chrom, stat)"
      ],
      "metadata": {
        "id": "chbtzAYSVkag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with official results (which use RC averaging)\n",
        "pd.read_csv(\"hf://datasets/songlab/TraitGym/mendelian_traits_matched_9/AUPRC_by_chrom_weighted_average/all/GPN_final_EuclideanDistance.plus.score.csv\")"
      ],
      "metadata": {
        "id": "HgvdWsZkV7sR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}